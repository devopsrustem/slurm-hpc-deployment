#
# Slurm cgroups Configuration
# Generated by Ansible for {{ slurmctld_options.cluster_name }}
# Date: {{ ansible_date_time.iso8601 }}
#
# This file controls resource isolation and enforcement using Linux cgroups
#

# CGROUP MOUNTING AND AUTOMATION
CgroupAutomount={{ cgroups_config.cgroup_automount }}
CgroupReleaseAgentDir={{ cgroups_config.cgroup_release_agent_dir }}

# RESOURCE CONSTRAINTS
ConstrainCores={{ cgroups_config.constrain_cores }}
ConstrainRAMSpace={{ cgroups_config.constrain_ram_space }}
ConstrainSwapSpace={{ cgroups_config.constrain_swap_space }}
ConstrainDevices={{ cgroups_config.constrain_devices }}
ConstrainKmemSpace={{ cgroups_config.constrain_kmem_space }}

# MEMORY MANAGEMENT
MemorySwappiness={{ cgroups_config.memory_swappiness }}
{% if cgroups_config.allowed_devices_file %}
AllowedDevicesFile={{ cgroups_config.allowed_devices_file }}
{% endif %}

# TASK/AFFINITY PLUGIN INTEGRATION
TaskAffinity=no
ConstrainRAMSpace=yes

# CGROUP MOUNTING OPTIONS
CgroupMountpoint="/sys/fs/cgroup"

# ALLOWED RAM PERCENTAGE
# Allow jobs to use slightly more RAM than allocated to handle overhead
AllowedRAMSpace=100.1

# ALLOWED SWAP PERCENTAGE  
# Disable swap usage in jobs for predictable performance
AllowedSwapSpace=0.0

# MAXIMUM RAM PERCENTAGE
# Hard limit - kill jobs that exceed this
MaxRAMPercent=95.0

# MAXIMUM SWAP PERCENTAGE
MaxSwapPercent=0.0

# MINIMUM RAM PERCENTAGE
# Minimum amount of RAM a job can request
MinRAMSpace=30

# CGROUP HIERARCHY
# For cgroup v1 systems
CgroupPlugin=cgroup/v1

# For systems with cgroup v2 (systemd)
# CgroupPlugin=cgroup/v2

# DEVICE ACCESS CONTROL
# Control which devices jobs can access
ConstrainDevices=yes

# IGNORE SYSTEM RESERVED MEMORY
# Account for system memory usage
IgnoreSystemd=yes
IgnoreSystemdOnFailure=yes

# KILL ON BAD EXIT
# Clean up cgroups when jobs exit abnormally
KillOnBadExit=yes

# CGROUP DEBUGGING
# Enable for troubleshooting (increases log verbosity)
# DebugFlags=CPU,Memory,Device

# STEP CGROUPS
# Create separate cgroups for job steps
# Useful for multi-step jobs and monitoring
TaskPlugin=task/cgroup
ProctrackType=proctrack/cgroup

# TMPFS CONFIGURATION
# Use tmpfs for job-specific temporary storage
JobContainerType=job_container/tmpfs

# CPU SET CONFIGURATION
# For NUMA-aware CPU allocation
# Automatically handled when ConstrainCores=yes

# MEMORY CONTROLLER CONFIGURATION
# Detailed memory control and monitoring
# Set appropriate memory limits based on requested resources

# NETWORK NAMESPACE ISOLATION
# For jobs requiring network isolation (advanced use case)
# ConstrainNetworkNamespace=no

# EXAMPLES AND NOTES:

# For GPU nodes, ensure GPU devices are accessible:
# Add to allowed_devices_file.conf:
# /dev/nvidia* rwm
# /dev/nvidiactl rwm
# /dev/nvidia-uvm rwm

# For InfiniBand access:
# /dev/infiniband/* rwm
# /dev/rdma_cm rwm

# For debugging cgroup issues:
# 1. Check /sys/fs/cgroup/slurm/ directory structure
# 2. Monitor: systemctl status slurmd
# 3. Check logs: /var/log/slurm/slurmd.log
# 4. Verify mount: mount | grep cgroup

# Memory enforcement example:
# If job requests 8GB, enforce 8.1GB limit (100.1% of requested)
# Kill job if it exceeds 95% of node memory (safety limit)

# CPU enforcement example:
# If job requests 4 cores, constrain to specific CPU set
# Use NUMA topology for optimal performance

# Performance tuning:
# - Disable swap (ConstrainSwapSpace=no, AllowedSwapSpace=0.0)
# - Use memory swappiness=1 for minimal swap usage
# - Enable device constraints for security
# - Monitor cgroup overhead in high-throughput environments

# Troubleshooting common issues:
# 1. Permission denied: Check cgroup mount permissions
# 2. Memory kills: Adjust AllowedRAMSpace or job memory requests
# 3. CPU binding issues: Verify ConstrainCores and CPU topology
# 4. Device access: Check AllowedDevicesFile configuration