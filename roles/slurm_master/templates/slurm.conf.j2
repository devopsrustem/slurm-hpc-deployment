#
# Slurm Configuration File
# Generated by Ansible for {{ slurmctld_options.cluster_name }}
# Date: {{ ansible_date_time.iso8601 }}
# Controller: {{ inventory_hostname }}
#

# CLUSTER IDENTIFICATION
ClusterName={{ slurmctld_options.cluster_name }}
ControlMachine={{ slurmctld_options.control_machine }}
{% if slurmctld_options.backup_controller %}
BackupController={{ slurmctld_options.backup_controller }}
{% endif %}

# SLURM USER AND DAEMON CONFIGURATION
SlurmUser={{ slurmctld_options.slurm_user }}
SlurmctldPort=6817
SlurmdPort=6818
SlurmdSpoolDir={{ slurm_spool_dir }}/d
StateSaveLocation={{ slurmctld_options.state_save_location }}
SlurmdPidFile={{ slurm_run_dir }}/slurmd.pid
SlurmctldPidFile={{ slurmctld_options.pid_file }}
SlurmdLogFile={{ slurm_log_dir }}/slurmd.log
SlurmctldLogFile={{ slurmctld_options.log_file }}

# AUTHENTICATION AND SECURITY
AuthType={{ slurmctld_options.auth_type }}
{% if slurmctld_options.auth_alt_types %}
AuthAltTypes={{ slurmctld_options.auth_alt_types }}
{% endif %}
{% if jwt_options.enabled %}
AuthAltParameters=jwt_key={{ jwt_options.key_file }}
{% endif %}
CredType={{ slurmctld_options.cred_type }}

# SCHEDULING CONFIGURATION
SchedulerType={{ slurmctld_options.scheduler_type }}
SelectType={{ slurmctld_options.select_type }}
SelectTypeParameters={{ slurmctld_options.select_type_parameters }}

# PERFORMANCE AND LIMITS
MaxJobCount={{ slurmctld_options.max_job_count }}
MaxArraySize={{ slurmctld_options.max_array_size }}
MaxStepCount={{ slurmctld_options.max_step_count }}
MaxTasksPerNode={{ slurmctld_options.max_tasks_per_node }}

# TIMEOUTS
SlurmdTimeout={{ slurmctld_options.slurmd_timeout }}
InactiveLimit={{ slurmctld_options.inactive_limit }}
MinJobAge={{ slurmctld_options.min_job_age }}
KillWait={{ slurmctld_options.kill_wait }}
CompleteWait={{ slurmctld_options.complete_wait }}

# NETWORKING AND TOPOLOGY
TreeWidth={{ slurmctld_options.tree_width }}
{% if slurmctld_options.switch_type %}
SwitchType={{ slurmctld_options.switch_type }}
{% endif %}
{% if slurmctld_options.topology_plugin %}
TopologyPlugin={{ slurmctld_options.topology_plugin }}
{% endif %}

# LOGGING AND DEBUGGING
SlurmctldDebug={{ slurmctld_options.slurmctld_debug }}
SlurmdDebug=3
DebugFlags=NO_CONF_HASH,BackfillSched

# SCHEDULING LOGS
{% if slurmctld_options.slurm_sched_log_file %}
SlurmSchedLogFile={{ slurmctld_options.slurm_sched_log_file }}
SlurmSchedLogLevel={{ slurmctld_options.slurm_sched_log_level }}
{% endif %}

# DATABASE CONFIGURATION
{% if database_host %}
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost={{ slurmctld_options.slurmdbd_host }}
AccountingStorageEnforce={{ slurm_accounting_storage_enforce }}
{% endif %}

# JOB ACCOUNTING
JobAcctGatherType={{ slurm_job_accounting_gather_type }}
JobAcctGatherFrequency={{ slurm_job_accounting_gather_frequency }}

# CGROUPS CONFIGURATION
TaskPlugin=task/cgroup
ProctrackType=proctrack/cgroup
JobContainerType=job_container/tmpfs

# RESOURCE CONSTRAINTS
EnforcePartLimits=ALL
OverTimeLimit=0

# PREEMPTION
PreemptType=preempt/none
PreemptMode=OFF

# POWER MANAGEMENT
SuspendProgram=/etc/slurm/suspend.sh
ResumeProgram=/etc/slurm/resume.sh
SuspendTime=NONE
ResumeTimeout=600
SuspendTimeout=30

# PROLOGS AND EPILOGS
Prolog=/etc/slurm/prolog.sh
Epilog=/etc/slurm/epilog.sh
PrologFlags=Alloc,Contain

# MPI CONFIGURATION
MpiDefault=none
MpiParams=ports=12000-12999

# GPU RESOURCE CONFIGURATION
GresTypes=gpu

# HEALTH CHECKING
HealthCheckProgram=/usr/sbin/nhc
HealthCheckInterval=300
HealthCheckNodeState=IDLE

# RETURN TO SERVICE
ReturnToService=1

# FAIR SHARE
PriorityType=priority/multifactor
PriorityDecayHalfLife=1-0
PriorityCalcPeriod=5
PriorityFavorSmall=NO
PriorityMaxAge=7-0
PriorityUsageResetPeriod=NONE
PriorityWeightAge=1000
PriorityWeightFairshare=10000
PriorityWeightJobSize=1000
PriorityWeightPartition=1000
PriorityWeightQOS=2000

# BURST BUFFER
BurstBufferType=burst_buffer/none

# CLUSTER NODES CONFIGURATION
# Note: Node and partition definitions should be added here
# This section will be populated automatically based on inventory

# Default compute nodes configuration (example)
{% if groups['slurm_compute'] is defined %}
{% for host in groups['slurm_compute'] %}
{% set host_facts = hostvars[host] %}
NodeName={{ host }} CPUs={{ host_facts.ansible_processor_vcpus | default(8) }} \
    Boards=1 \
    SocketsPerBoard={{ host_facts.ansible_processor_count | default(2) }} \
    CoresPerSocket={{ (host_facts.ansible_processor_vcpus | default(8) // host_facts.ansible_processor_count | default(2)) }} \
    ThreadsPerCore={{ host_facts.ansible_processor_threads_per_core | default(1) }} \
    RealMemory={{ (host_facts.ansible_memtotal_mb | default(32768) * 0.95) | int }} \
    TmpDisk=10000 \
{% if host_facts.gpu_count is defined %}
    Gres=gpu:{{ host_facts.gpu_type | default('generic') }}:{{ host_facts.gpu_count | default(8) }} \
{% endif %}
    State=UNKNOWN
{% endfor %}

# Partition configuration
PartitionName=batch Nodes={{ groups['slurm_compute'] | join(',') }} Default=YES MaxTime=INFINITE State=UP \
    DefaultTime=01:00:00 \
    MaxNodes={{ groups['slurm_compute'] | length }} \
    OverSubscribe=NO \
    PriorityJobFactor=1 \
    PriorityTier=1
{% endif %}

# GPU partition (if GPU nodes exist)
{% if groups['gpu_nodes'] is defined %}
PartitionName=gpu Nodes={{ groups['gpu_nodes'] | join(',') }} Default=NO MaxTime=7-00:00:00 State=UP \
    DefaultTime=01:00:00 \
    MaxNodes={{ groups['gpu_nodes'] | length }} \
    OverSubscribe=NO \
    PriorityJobFactor=2 \
    PriorityTier=2
{% endif %}

# High priority partition for short jobs
{% if groups['slurm_compute'] is defined and groups['slurm_compute'] | length >= 4 %}
PartitionName=express Nodes={{ groups['slurm_compute'][:4] | join(',') }} Default=NO MaxTime=01:00:00 State=UP \
    DefaultTime=00:15:00 \
    MaxNodes=2 \
    OverSubscribe=YES \
    PriorityJobFactor=10 \
    PriorityTier=10
{% endif %}

# Debug partition for testing
{% if groups['slurm_compute'] is defined and groups['slurm_compute'] | length >= 1 %}
PartitionName=debug Nodes={{ groups['slurm_compute'][0] }} Default=NO MaxTime=00:30:00 State=UP \
    DefaultTime=00:10:00 \
    MaxNodes=1 \
    OverSubscribe=YES \
    PriorityJobFactor=100 \
    PriorityTier=100
{% endif %}

# INCLUDE ADDITIONAL CONFIGURATIONS
include /etc/slurm/slurm.d/*.conf