---
# =============================================================================
# ИНИЦИАЛИЗАЦИЯ SLURM КЛАСТЕРА
# =============================================================================

- name: "Ожидание готовности всех сервисов перед инициализацией"
  wait_for:
    host: "{{slurm_master_address}}"
    port: "{{item}}"
    timeout: 120
  loop:
  - "{{slurm_slurmctld_port}}"
  - "{{slurm_slurmdbd_port if slurm_enable_slurmdbd | default(true) else omit}}"
  when: item != omit
  tags: [cluster-init, dependencies]

- name: "Проверка доступности Slurm команд"
  command: "{{slurm_install_prefix}}/bin/{{item}} --version"
  register: slurm_commands_check
  failed_when: slurm_commands_check.rc != 0
  changed_when: false
  loop:
  - sinfo
  - squeue
  - sacctmgr
  - scontrol
  tags: [cluster-init, validation]

- name: "Инициализация базы данных кластера"
  block:
- name: "Проверка существования кластера в базе данных"
    command: >
      {{slurm_install_prefix}}/bin/sacctmgr  -n -p list cluster {{slurm_cluster_name}}
    register: cluster_exists
    changed_when: false
    failed_when: false

- name: "Создание кластера в базе данных"
    command: >
      {{slurm_install_prefix}}/bin/sacctmgr  -i add cluster {{slurm_cluster_name}}
    when: cluster_exists.stdout == ""
    register: cluster_creation

- name: "Подтверждение создания кластера"
    debug:
      msg: "Кластер {{slurm_cluster_name}} успешно создан в базе данных"
    when: cluster_creation is changed

  when: slurm_enable_slurmdbd | default(true)
  tags: [cluster-init, database]

- name: "Инициализация учетных записей и QoS"
  block:
- name: "Создание корневой учетной записи (root account)"
    command: >
      {{slurm_install_prefix}}/bin/sacctmgr  -i add account {{slurm_root_account | default('root')}} Description="Root account for cluster {{slurm_cluster_name}}" Organization="{{slurm_organization | default('HPC Cluster')}}"
    register: root_account_creation
    failed_when: root_account_creation.rc != 0 and 'already exists' not in root_account_creation.stderr

- name: "Создание дефолтных учетных записей"
    command: >
      {{slurm_install_prefix}}/bin/sacctmgr  -i add account {{item.name}} Description="{{item.description | default('Account ' + item.name)}}" {% if item.parent is defined %}Parent={{item.parent}}{% endif %} {% if item.organization is defined %}Organization="{{item.organization}}"{% endif %}
    loop: "{{slurm_default_accounts | default([])}}"
    register: accounts_creation
    failed_when: accounts_creation.rc != 0 and 'already exists' not in accounts_creation.stderr

- name: "Создание QoS (Quality of Service) политик"
    command: >
      {{slurm_install_prefix}}/bin/sacctmgr  -i add qos {{item.name}} {% if item.description is defined %}Description="{{item.description}}"{% endif %} {% if item.max_wall is defined %}MaxWall={{item.max_wall}}{% endif %} {% if item.max_jobs is defined %}MaxJobs={{item.max_jobs}}{% endif %} {% if item.max_submit is defined %}MaxSubmit={{item.max_submit}}{% endif %} {% if item.max_nodes is defined %}MaxNodes={{item.max_nodes}}{% endif %} {% if item.max_cpus is defined %}MaxCPUs={{item.max_cpus}}{% endif %} {% if item.priority is defined %}Priority={{item.priority}}{% endif %}
    loop: "{{slurm_qos_policies | default([])}}"
    register: qos_creation
    failed_when: qos_creation.rc != 0 and 'already exists' not in qos_creation.stderr

- name: "Создание пользователей в системе учета"
    command: >
      {{slurm_install_prefix}}/bin/sacctmgr  -i add user {{item.name}} Account={{item.account | default(slurm_root_account | default('root'))}} {% if item.admin_level is defined %}AdminLevel={{item.admin_level}}{% endif %} {% if item.default_account is defined %}DefaultAccount={{item.default_account}}{% endif %} {% if item.qos is defined %}QoS={{item.qos}}{% endif %}
    loop: "{{slurm_cluster_users | default([])}}"
    register: users_creation
    failed_when: users_creation.rc != 0 and 'already exists' not in users_creation.stderr

  when: slurm_enable_slurmdbd | default(true)
  tags: [cluster-init, accounts, qos]

- name: "Инициализация узлов кластера"
  block:
- name: "Проверка текущего состояния узлов"
    command: "{{slurm_install_prefix}}/bin/sinfo -N -h -o '%N %T'"
    register: nodes_status
    changed_when: false

- name: "Отображение состояния узлов"
    debug:
      msg: |
        Текущее состояние узлов:
        {{nodes_status.stdout_lines | join('\n')}}

- name: "Установка узлов в состояние IDLE (если они в UNKNOWN)"
    command: >
      {{slurm_install_prefix}}/bin/scontrol  update NodeName={{item}} State=RESUME
    loop: "{{slurm_all_compute_nodes | default([])}}"
    when:
    - item + ' unknown' in nodes_status.stdout or item + ' down' in nodes_status.stdout
    register: nodes_resume
    failed_when: false

- name: "Ожидание перехода узлов в рабочее состояние"
    command: "{{slurm_install_prefix}}/bin/sinfo -N -h -t idle,alloc,mix,comp -o '%N'"
    register: ready_nodes
    retries: 10
    delay: 30
    until: ready_nodes.stdout_lines | length > 0
    when: slurm_wait_for_nodes | default(true)

  tags: [cluster-init, nodes]

- name: "Создание тестовых разделов и заданий"
  block:
- name: "Создание тестового задания"
    copy:
      content: |
        #!/bin/bash
        #SBATCH --job-name=cluster-init-test
        #SBATCH --output={{slurm_log_dir}}/cluster-init-test.out
        #SBATCH --error={{slurm_log_dir}}/cluster-init-test.err
        #SBATCH --time=00:01:00
        #SBATCH --ntasks=1
        #SBATCH --cpus-per-task=1

        echo "Cluster initialization test started at $(date)"
        echo "Hostname: $(hostname)"
        echo "Slurm job ID: $SLURM_JOB_ID"
        echo "Slurm partition: $SLURM_JOB_PARTITION"
        echo "CPUs allocated: $SLURM_CPUS_PER_TASK"
        echo "Memory allocated: $SLURM_MEM_PER_NODE MB"

        # Test basic system commands
        echo "System information:"
        uname -a
        cat /proc/version

        # Test CPU info
        echo "CPU information:"
        lscpu | head -20

        # Test memory info
        echo "Memory information:"
        free -h

        # Test GPU detection (if available)
        if command -v nvidia-smi &> /dev/null; then
            echo "GPU information:"
            nvidia-smi -L
        else
            echo "No NVIDIA GPUs detected or nvidia-smi not available"
        fi

        echo "Cluster initialization test completed at $(date)"
      dest: "{{slurm_config_dir}}/cluster-init-test.sh"
      mode: "0755"
      owner: "{{slurm_user}}"
      group: "{{slurm_group}}"

- name: "Запуск тестового задания"
    command: >
      {{slurm_install_prefix}}/bin/sbatch  {{slurm_config_dir}}/cluster-init-test.sh
    register: test_job_submit
    become_user: "{{slurm_user}}"
    when: slurm_run_init_test | default(true)

- name: "Получение ID тестового задания"
    set_fact:
      test_job_id: "{{test_job_submit.stdout | regex_search('\\d+')}}"
    when: test_job_submit is defined and test_job_submit.rc == 0

- name: "Ожидание завершения тестового задания"
    command: >
      {{slurm_install_prefix}}/bin/squeue  -j {{test_job_id}} -h -o '%T'
    register: test_job_status
    retries: 20
    delay: 30
    until: test_job_status.stdout == '' or 'COMPLETED' in test_job_status.stdout
    when: test_job_id is defined

- name: "Проверка результатов тестового задания"
    command: >
      {{slurm_install_prefix}}/bin/sacct  -j {{test_job_id}} -o JobID,JobName,State,ExitCode,Start,End,NodeList
    register: test_job_results
    when: test_job_id is defined

- name: "Отображение результатов тестового задания"
    debug:
      msg: |
        Результаты тестового задания:
        {{test_job_results.stdout_lines | join('\n')}}
    when: test_job_results is defined

  when: slurm_run_cluster_tests | default(true)
  tags: [cluster-init, testing]

- name: "Создание скриптов управления кластером"
  copy:
    content: |
      #!/bin/bash
      # {{item.description}}
      {{item.content}}
    dest: "{{item.dest}}"
    mode: "0755"
    owner: root
    group: root
  loop:
  - description: "Cluster Status Script"
    dest: "{{slurm_install_prefix}}/bin/cluster-status"
    content: |
      echo "=== Slurm Cluster Status ==="
      {{slurm_install_prefix}}/bin/sinfo
      echo ""
      echo "=== Queue Status ==="
      {{slurm_install_prefix}}/bin/squeue
      echo ""
      echo "=== Node Details ==="
      {{slurm_install_prefix}}/bin/scontrol show nodes | head -20
  - description: "Cluster Health Script"
    dest: "{{slurm_install_prefix}}/bin/cluster-health"
    content: |
      echo "=== Cluster Health Check ==="
      echo "Services:"
      systemctl is-active munge slurmctld slurmdbd --quiet && echo "✓ All services running" || echo "✗ Some services down"
      echo ""
      echo "Nodes:"
      DOWN_NODES=$( {{slurm_install_prefix}}/bin/sinfo -h -t down -o "%N" | wc -l)
      if ["$DOWN_NODES" -eq 0]; then
        echo "✓ No nodes down"
      else
        echo "✗ $DOWN_NODES nodes down"
      fi
  tags: [cluster-init, scripts]

- name: "Создание документации кластера"
  copy:
    content: |
      # {{slurm_cluster_name}} Cluster Information

      ## Cluster Details
      - **Name**: {{slurm_cluster_name}}
      - **Master Node**: {{slurm_master_node}}
      - **Initialized**: {{ansible_date_time.iso8601}}
      - **Slurm Version**: {{slurm_version | default('25.05.1')}}

      ## Services
      - **MUNGE**: Authentication service
      {% if slurm_enable_slurmdbd | default(true) %}
      - **SLURMDBD**: Database daemon
      {% endif %}
      - **SLURMCTLD**: Controller daemon
      {% if slurm_enable_rest_api | default(false) %}
      - **SLURMRESTD**: REST API service
      {% endif %}

      ## Management Commands
      ```bash
      # Cluster status
      {{slurm_install_prefix}}/bin/cluster-status

      # Service control
      {{slurm_install_prefix}}/bin/slurm-services [start|stop|restart|status]

      # View jobs and nodes
      {{slurm_install_prefix}}/bin/sinfo
      {{slurm_install_prefix}}/bin/squeue
      ```

      ## Configuration Files
      - Main config: {{slurm_config_dir}}/slurm.conf
      - GRES config: {{slurm_config_dir}}/gres.conf
      - Cgroup config: {{slurm_config_dir}}/cgroup.conf
    dest: "{{slurm_config_dir}}/cluster-info.md"
    mode: "0644"
    owner: "{{slurm_user}}"
    group: "{{slurm_group}}"
  tags: [cluster-init, documentation]

- name: "Настройка автоматических задач обслуживания"
  cron:
    name: "{{item.name}}"
    minute: "{{item.minute | default('*')}}"
    hour: "{{item.hour | default('*')}}"
    day: "{{item.day | default('*')}}"
    month: "{{item.month | default('*')}}"
    weekday: "{{item.weekday | default('*')}}"
    user: root
    job: "{{item.job}}"
    state: "{{'present' if slurm_enable_maintenance_tasks | default(true) else 'absent'}}"
  loop:
- name: "Cluster health check"
    minute: "*/15"
    job: "{{slurm_install_prefix}}/bin/cluster-health > /dev/null 2>&1"
- name: "Clean old job logs"
    hour: "2"
    minute: "0"
    job: "find {{slurm_log_dir}} -name '*.out' -mtime +7 -delete"
- name: "Database maintenance"
    hour: "3"
    minute: "0"
    weekday: "0"
    job: "{{slurm_install_prefix}}/bin/sacctmgr -i archive"
  tags: [cluster-init, maintenance, cron]

- name: "Финальная проверка инициализации кластера"
  block:
- name: "Проверка статуса кластера"
    command: "{{slurm_install_prefix}}/bin/sinfo"
    register: final_cluster_status
    changed_when: false

- name: "Проверка очереди заданий"
    command: "{{slurm_install_prefix}}/bin/squeue"
    register: final_queue_status
    changed_when: false

- name: "Проверка учетной записи кластера"
    command: >
      {{slurm_install_prefix}}/bin/sacctmgr  -n show cluster {{slurm_cluster_name}}
    register: final_account_status
    changed_when: false
    when: slurm_enable_slurmdbd | default(true)

- name: "Отображение финального статуса"
    debug:
      msg: |
        ================================================================
        ИНИЦИАЛИЗАЦИЯ КЛАСТЕРА ЗАВЕРШЕНА
        ================================================================

        Состояние кластера:
        {{final_cluster_status.stdout_lines | join('\n')}}

        Очередь заданий:
        {{final_queue_status.stdout_lines | join('\n') if final_queue_status.stdout_lines | length > 0 else 'Очередь пуста'}}

        {% if slurm_enable_slurmdbd | default(true) %}
        Учетная запись кластера:
        {{final_account_status.stdout_lines | join('\n') if final_account_status.stdout_lines | length > 0 else 'Кластер зарегистрирован'}}
        {% endif %}

        Управляющие команды:
        - Статус кластера: {{slurm_install_prefix}}/bin/cluster-status
        - Здоровье кластера: {{slurm_install_prefix}}/bin/cluster-health
        - Приостановка узлов: {{slurm_install_prefix}}/bin/cluster-drain
        - Возобновление узлов: {{slurm_install_prefix}}/bin/cluster-resume

        Кластер {{slurm_cluster_name}} готов к работе!
        ================================================================

  tags: [cluster-init, validation, summary]

- name: "Создание файла состояния инициализации"
  copy:
    content: |
      Cluster: {{slurm_cluster_name}}
      Initialized: {{ansible_date_time.iso8601}}
      Version: {{slurm_version | default('25.05.1')}}
      Master node: {{slurm_master_node}}
      Database enabled: {{slurm_enable_slurmdbd | default(true)}}
      REST API enabled: {{slurm_enable_rest_api | default(false)}}
      JWT auth enabled: {{slurm_enable_jwt_auth | default(false)}}
      Status: READY
    dest: "{{slurm_config_dir}}/cluster-initialized"
    owner: "{{slurm_user}}"
    group: "{{slurm_group}}"
    mode: "0644"
  tags: [cluster-init, status]
