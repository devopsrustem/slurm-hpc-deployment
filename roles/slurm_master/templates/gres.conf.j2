#
# Slurm GRES (Generic Resource) Configuration
# Generated by Ansible for {{ slurmctld_options.cluster_name }}
# Date: {{ ansible_date_time.iso8601 }}
#
# This file defines GPU resources available on each node
#

# AutoDetect configuration for automatic GPU detection
# Uncomment for automatic detection (requires NVML support)
# AutoDetect=nvml

{% if groups['slurm_compute'] is defined %}
{% for host in groups['slurm_compute'] %}
{% set host_facts = hostvars[host] %}
{% if host_facts.gpu_count is defined and host_facts.gpu_count > 0 %}

# {{ host }} - {{ host_facts.gpu_type | default('Unknown GPU') }} GPUs
{% if host_facts.gpu_type == 'h100' %}
# NVIDIA H100 GPUs configuration
{% for gpu_index in range(host_facts.gpu_count | default(8)) %}
NodeName={{ host }} Name=gpu Type={{ host_facts.gpu_type }} File=/dev/nvidia{{ gpu_index }}{% if host_facts.gpu_cores_mapping is defined %} Cores={{ host_facts.gpu_cores_mapping[gpu_index] | default(loop.index0 * 16) }}-{{ host_facts.gpu_cores_mapping[gpu_index] | default(loop.index0 * 16) + 15 }}{% endif %}

{% endfor %}
{% elif host_facts.gpu_type == 'a100' %}
# NVIDIA A100 GPUs configuration
{% for gpu_index in range(host_facts.gpu_count | default(8)) %}
NodeName={{ host }} Name=gpu Type={{ host_facts.gpu_type }} File=/dev/nvidia{{ gpu_index }}{% if host_facts.gpu_cores_mapping is defined %} Cores={{ host_facts.gpu_cores_mapping[gpu_index] | default(loop.index0 * 8) }}-{{ host_facts.gpu_cores_mapping[gpu_index] | default(loop.index0 * 8) + 7 }}{% endif %}

{% endfor %}
{% else %}
# Generic GPU configuration for {{ host }}
{% for gpu_index in range(host_facts.gpu_count | default(8)) %}
NodeName={{ host }} Name=gpu Type={{ host_facts.gpu_type | default('generic') }} File=/dev/nvidia{{ gpu_index }}

{% endfor %}
{% endif %}
{% endif %}
{% endfor %}
{% endif %}

# Example configurations for different GPU types:

# For DGX H100 systems (8x H100 GPUs)
# Each GPU mapped to specific CPU cores for optimal NUMA locality
# NodeName=cn01 Name=gpu Type=h100 File=/dev/nvidia0 Cores=0-15
# NodeName=cn01 Name=gpu Type=h100 File=/dev/nvidia1 Cores=16-31
# NodeName=cn01 Name=gpu Type=h100 File=/dev/nvidia2 Cores=32-47
# NodeName=cn01 Name=gpu Type=h100 File=/dev/nvidia3 Cores=48-63
# NodeName=cn01 Name=gpu Type=h100 File=/dev/nvidia4 Cores=64-79
# NodeName=cn01 Name=gpu Type=h100 File=/dev/nvidia5 Cores=80-95
# NodeName=cn01 Name=gpu Type=h100 File=/dev/nvidia6 Cores=96-111
# NodeName=cn01 Name=gpu Type=h100 File=/dev/nvidia7 Cores=112-127

# For DGX A100 systems (8x A100 GPUs)
# NodeName=cn02 Name=gpu Type=a100 File=/dev/nvidia0 Cores=0-7
# NodeName=cn02 Name=gpu Type=a100 File=/dev/nvidia1 Cores=8-15
# NodeName=cn02 Name=gpu Type=a100 File=/dev/nvidia2 Cores=16-23
# NodeName=cn02 Name=gpu Type=a100 File=/dev/nvidia3 Cores=24-31
# NodeName=cn02 Name=gpu Type=a100 File=/dev/nvidia4 Cores=32-39
# NodeName=cn02 Name=gpu Type=a100 File=/dev/nvidia5 Cores=40-47
# NodeName=cn02 Name=gpu Type=a100 File=/dev/nvidia6 Cores=48-55
# NodeName=cn02 Name=gpu Type=a100 File=/dev/nvidia7 Cores=56-63

# For systems with MIG (Multi-Instance GPU) configuration:
# NodeName=cn03 Name=gpu Type=a100_1g.5gb File=/dev/nvidia0 Count=7
# NodeName=cn03 Name=gpu Type=a100_3g.20gb File=/dev/nvidia0 Count=2
# NodeName=cn03 Name=gpu Type=a100_7g.40gb File=/dev/nvidia0 Count=1

# For mixed GPU environments:
# NodeName=cn04 Name=gpu Type=rtx4090 File=/dev/nvidia0,/dev/nvidia1
# NodeName=cn05 Name=gpu Type=v100 File=/dev/nvidia0 Cores=0-7

# Advanced configuration with Links (NVLink topology):
# NodeName=cn01 Name=gpu Type=h100 File=/dev/nvidia0 Links=1,2,3
# NodeName=cn01 Name=gpu Type=h100 File=/dev/nvidia1 Links=0,2,3
# NodeName=cn01 Name=gpu Type=h100 File=/dev/nvidia2 Links=0,1,3
# NodeName=cn01 Name=gpu Type=h100 File=/dev/nvidia3 Links=0,1,2

# For nodes without GPUs (CPU-only):
# NodeName=login01 Name=gpu Count=0

# Notes:
# - Use 'nvidia-smi topo -m' to determine optimal CPU core mapping
# - Use 'nvidia-ml-py' or 'nvidia-smi' to get GPU topology information
# - For production, consider using AutoDetect=nvml for automatic configuration
# - GRES scheduling requires proper CPU affinity for optimal performance

# CPU affinity examples for different architectures:
# Intel Xeon with 2 sockets, hyperthreading disabled:
# - Socket 0: Cores 0-31, Socket 1: Cores 32-63
# AMD EPYC with 2 sockets, hyperthreading disabled:
# - Socket 0: Cores 0-63, Socket 1: Cores 64-127

# For debugging GRES configuration:
# 1. Check Slurm logs: /var/log/slurm/slurmd.log
# 2. Verify with: scontrol show node <nodename>
# 3. Test with: srun --gres=gpu:1 nvidia-smi
# 4. Check GPU topology: nvidia-smi topo -m